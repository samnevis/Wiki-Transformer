# Wikipedia Transformer

A PyTorch implementation of a transformer-based language model trained on Wikipedia text. This project is inspired by the nanoGPT architecture but simplified for educational purposes.

## Overview

This project implements a transformer-based language model that can:
- Train on Wikipedia text data
- Generate text based on user prompts
- Save and load model checkpoints

The model architecture is based on the transformer architecture with:
- 6 transformer layers
- 6 attention heads
- 384 embedding dimensions
- 1536 feedforward dimensions

## Features

- **Iteration-based Training**: Uses iterations instead of epochs for more flexible training
- **Gradient Accumulation**: Implements gradient accumulation for effective larger batch sizes
- **Learning Rate Scheduling**: Includes warmup and cosine decay for better convergence
- **Mixed Precision Training**: Uses automatic mixed precision for faster training
- **Checkpoint Saving**: Saves the best model based on loss
- **Text Generation**: Generates text from user prompts with configurable parameters

## Requirements

- Python 3.8+
- PyTorch 2.0+
- Hugging Face datasets
- tiktoken (OpenAI's tokenizer)

## Installation

1. Clone this repository
2. Install the required packages:
   ```
   pip install torch datasets tiktoken tqdm
   ```

## Usage

### Training

To train the model on Wikipedia data:

```
python wikipedia_transformer.py --mode train
```

The training process will:
- Load Wikipedia data
- Process it into training examples
- Train the model for 60,000 iterations
- Save checkpoints when the loss improves

### Text Generation

To generate text using the trained model:

```
python wikipedia_transformer.py --mode generate --prompt "Your prompt here"
```

You can customize the generation parameters in the CONFIG dictionary:
- `temperature`: Controls randomness (higher = more random)
- `top_k`: Number of highest probability tokens to keep
- `top_p`: Cumulative probability threshold for nucleus sampling
- `max_new_tokens`: Maximum number of tokens to generate

#### Example Generation

Here's an example of text generated by the model with the prompt "The best thing is":

```
The best thing is the attribute of the god, according to Homer from the greatest of the Iliad. In the earliest Greek, he is the most important attribute of the life, especially in the Roman form of the Greek language of the poem.

Aristotle was credited with Phoenicians across the late 18th century. It was considered the basis of Apollo's Egypt and Homer's son Asperipi observed to flee to his mother, and he also gave the Niobids until his death in Troy
```

This example demonstrates the model's ability to generate coherent text that follows the style and content patterns found in Wikipedia articles, particularly those about historical and mythological topics.

## Model Architecture

The model uses a standard transformer architecture with:
- Multi-head self-attention
- Position-wise feedforward networks
- Layer normalization
- Residual connections
- Dropout for regularization

## Training Progress

The model has been trained for 16,000 iterations with a training loss of 1.0533. This indicates good progress in learning the patterns in the Wikipedia text.

## Future Improvements

Potential areas for improvement:
- Add validation set for better model selection
- Implement more sophisticated sampling strategies
- Add support for fine-tuning on specific domains
- Implement model quantization for inference

## License

This project is open source and available under the MIT License.

## Acknowledgments

- Inspired by the [nanoGPT](https://github.com/karpathy/nanoGPT) project
- Uses the [Hugging Face datasets](https://huggingface.co/datasets) library
- Uses OpenAI's [tiktoken](https://github.com/openai/tiktoken) for tokenization 